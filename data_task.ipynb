{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "842d12ae-5ed7-4845-8021-7ad2aada18e1",
   "metadata": {},
   "source": [
    "# data analysis task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68744940-dbfe-4588-9028-aaa00b2f2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7952652e-9773-4fb1-b628-e8163184ded6",
   "metadata": {},
   "source": [
    "## Read in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c17f2c5-68f1-42ed-a99f-6c5c97330677",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b714d7fb-874b-46e7-a145-022207b2c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some basic cleaning as column headers have trailing whitespace, clear these up\n",
    "print(f\"Before: {df.columns}\")\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "print(f\"After: {df.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18855de4-5d6d-4a96-8e46-80292abbe508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e16b948b-977a-422c-918d-a74cde4aa35e",
   "metadata": {},
   "source": [
    "## Test/train split to avoid peaking during exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c10aeda-9f23-4764-aa80-447c6bad1bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify target column name\n",
    "target_col = 'Out'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c113e8-2954-4bd2-9de3-4e19c4614af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cols = [c for c in df.columns if c != target_col]\n",
    "\n",
    "df_train_features, df_test_features, df_train_target, df_test_target = train_test_split(\n",
    "    df[train_cols], \n",
    "    df[target_col], \n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7598bfd8-6a4d-4c0d-897a-4d60fc5ed8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2945a591-ba4c-4421-8277-87ddf1ffe73a",
   "metadata": {},
   "source": [
    "## Initial exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a14416-9bd2-4188-9029-3af215ca0353",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_features.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101b7e4f-d77c-43f0-b9fe-2d62635ea143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get an overview of descriptive statistics of the data\n",
    "df_train_features.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773abceb-7e18-46ed-9be2-a1e3aeda02ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values in an cell\n",
    "df_train_features.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268a2f41-d125-4e1b-9251-53beced95a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56beed1c-210b-4b25-a6dc-511955f0e7c9",
   "metadata": {},
   "source": [
    "## Make some descriptive plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9264ea4c-56b8-4cfe-9a20-70e5d45bde31",
   "metadata": {},
   "source": [
    "### Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a51fcbe7-5f37-4bcd-baf8-483610f51778",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,9))\n",
    "i = 0\n",
    "for col in df_train_features.columns:\n",
    "    i+=1\n",
    "    plt.subplot(3,4,i)\n",
    "    plt.plot(df_train_features[col], \".\")\n",
    "    plt.title(col)\n",
    "    if i == 1:\n",
    "        plt.ylabel('raw plots')\n",
    "    \n",
    "    plt.subplot(3,4,i+4)\n",
    "    plt.plot(np.sort(df_train_features[col]), \".\")\n",
    "    if i+4 == 5:\n",
    "        plt.ylabel('sorted')\n",
    "    \n",
    "    plt.subplot(3,4,i+8)\n",
    "    plt.hist(df_train_features[col])\n",
    "    if i+8 == 9:\n",
    "        plt.ylabel('histograms')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb553c73-f5f5-4537-8da5-915df628b76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"% of rows with In2 -1 value: {(df_train_features['In2'] == -1).mean() * 100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faffd5f4-bc63-4cdd-a7e2-7445bc82f1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e53c67ae-ae48-4370-a246-3b48157c75f4",
   "metadata": {},
   "source": [
    "### Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8569aadc-558e-4c77-b629-8323766e3079",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,2))\n",
    "\n",
    "plt.subplot(1,3,1)\n",
    "plt.plot(np.sort(df_train_target), \".\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.plot(np.sort(df_train_target), \".\")\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.hist(df_train_target,bins=10)\n",
    "plt.yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc48348-7487-4df1-a94e-675f55d7bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"% of rows with outlier target: {(df_train_target > 10000).mean() * 100}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af53abf-d561-49b9-9887-ee5ecff0c7ef",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "- Features In1 and In4 are binary\n",
    "- In3 seems to follow a uniform distribution\n",
    "- In2 follows also follows a uniform distribution with ~2% of values as -1.\n",
    "- The target column has a strange distribution with 75% following a logarithmic growth (when plotted in order) with a step and then a few (~4) outliers\n",
    "\n",
    "Without knowing more about the data and what is being measured it is difficult to know how to treat the outliers in the target. My experience would say that these are either errors in the measuring method and these rows should be discarded. However, it's possible that these are valid, but rare events which may be the most important to predict.\n",
    "\n",
    "Likewise, the In2 values with -1: My instinct is that these are errors in a sensor, however they could be meaningful values that are indicative of a non-linearity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada57cb3-0d2d-4317-a2f5-592dd5ecd485",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87baaefa-2e1e-4669-9b5d-5c5e46742855",
   "metadata": {},
   "source": [
    "## Look at the relationship between plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaae0ce-77bd-4613-b5c7-98da9668764d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,7))\n",
    "sns.pairplot(\n",
    "    pd.merge(\n",
    "        df_train_features,\n",
    "        np.log10(df_train_target),\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    ),\n",
    "    height=1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945ee08a-0e57-4c7c-8ad5-4f181519275d",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "- Doesn't seem to be much correlation between any of the features, so no need to remove features or look for principal components / other dimensionality reduction methods\n",
    "- In2 does seem to have some relationship with the 'logarithmic' growth portion of the target\n",
    "- It doesn't look like the -1 In2 values are predictive of the Out outlier values\n",
    "- Feature scaling of In2 and In3 to get these into the same range may be beneficial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce413d3e-3d3c-4431-98fa-0338885fac59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d180e9bb-5b02-4582-a2ef-3200e2e1207f",
   "metadata": {},
   "source": [
    "## Modelling Approach\n",
    "\n",
    "The modelling approach crucially depends on how we want to treat the outliers in the target.\n",
    "\n",
    "_Scenario 1_: If we believe them to be erroneous then we can just remove any rows where the 'Out' value is greater than 10,000 (in both the training and test sets).  \n",
    "\n",
    "_Scenario 2_: If we believe that these are true values then I think that a linear model will really struggle to fit the data. We can do a few things such as fitting the log of the 'Out' values as opposed to the true values, trying MAE as well as MSE, but ultimately I think this big non-linearity will require a non-linear model such as a random forest. Most likely I would approach the problem with an ensemble approach:\n",
    "- classification/outlier detection\n",
    "- regression on non-outlier data\n",
    "\n",
    "In the brief it said that the the data has not been preprocessed and that the data can be fit without advanced machine learning algorithms. This makes me lean towards assuming that the data has erroneous outliers that should be removed. So I am going to make this assumption here.\n",
    "\n",
    "Likewise, we can either treat the In2 values that are equal to -1 as indicative of bad data and just remove these rows, or we can treat these as a feature - perhaps creating a new binary feature that acts as an idicator for a -1 value and perhaps capture more of the non-linearity aspect of this feature allowing for the linear values to be considered as they are. \n",
    "\n",
    "## Feature Engineering\n",
    "\n",
    "Don't know much about these features, but I think it would be interesting to see if creating more features that capture their interactions may be additionally predcitive.\n",
    "\n",
    "I can also create a feature that acts as an indicator for -1 values in 'In2'. \n",
    "\n",
    "Feature scaling is probably also a good idea as In2 is an order of magnitude less than In3 and the other two features are discrete 0-1 values. In2 and In3 are both uniformly distributed so a min-max scaler seems more appropriate than normalizing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b45585a-0707-4355-beae-f9c4cfdb06fb",
   "metadata": {},
   "source": [
    "## Feature / Target engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9841a5-af71-4070-b92b-aedd3422c9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get interaction features\n",
    "\n",
    "def make_interaction_features_df(df_in):\n",
    "    \n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    interactions = poly.fit_transform(df_in)\n",
    "\n",
    "    n_original_features = df_in.shape[1]\n",
    "    df_interactions = pd.DataFrame(\n",
    "        data=interactions[:,n_original_features:], \n",
    "        index=df_in.index,\n",
    "        columns=poly.get_feature_names_out()[n_original_features:]\n",
    "    )\n",
    "    \n",
    "    return df_interactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0c4ffc-2bd0-4d00-968e-5ea64bdd096c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create boolean feature for the In2 -1 value\n",
    "\n",
    "def make_In2_non_linear_bool_feature_df(df_in):\n",
    "    \n",
    "    bool_vector = df_in['In2'] < 0\n",
    "    \n",
    "    return pd.DataFrame(bool_vector).rename({'In2': 'In2_bool'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8214e58-c77a-449b-a2d6-bd765df181ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the rows from target and features with bad out rows\n",
    "\n",
    "def remove_bad_rows_out(df_features_in, df_target_in, threshold):\n",
    "    \n",
    "    good_idx = df_target_in.loc[df_target_in < 10000].index.tolist()\n",
    "    \n",
    "    df_features_out = df_features_in.loc[good_idx].copy()\n",
    "    df_target_out = df_target_in.loc[good_idx].copy()\n",
    "    \n",
    "    return df_features_out, df_target_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71032931-e63d-4624-bc2b-7c226c523a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows from features and target with bad In2 values\n",
    "\n",
    "def remove_bad_In2_rows(df_features_in, df_target_in):\n",
    "    \n",
    "    good_idx = df_features_in.loc[df_features_in['In2'] != -1].index.tolist()\n",
    "    \n",
    "    df_features_out = df_features_in.loc[good_idx].copy()\n",
    "    df_target_out = df_target_in.loc[good_idx].copy()\n",
    "    \n",
    "    return df_features_out, df_target_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cfa8abb-232a-4896-aa7c-1c41702809ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f46e9ec3-d7dc-4a6a-b2ab-4a4de63b6659",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "Run a set of experiments with several models and optional preprocessing and feature engineering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081fb7ac-3d1a-48c6-8bd1-f1836a42722d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "las_reg = Lasso()\n",
    "rf_reg = RandomForestRegressor()\n",
    "\n",
    "options = {\n",
    "    'model': [\n",
    "        lin_reg, \n",
    "        las_reg, \n",
    "        rf_reg,\n",
    "    ],\n",
    "    'data_cleaning': [\n",
    "        'none',\n",
    "        'remove_bad_In2_rows'\n",
    "    ],\n",
    "    'scaling': [\n",
    "        True,\n",
    "        False,\n",
    "    ],\n",
    "    'additional_features': [\n",
    "        'none',\n",
    "        'interaction',\n",
    "        'In2_bool',\n",
    "        'interaction,In2_bool',\n",
    "    ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffae1201-a0d5-4823-8660-9beedbe85a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = []\n",
    "for model in options['model']:\n",
    "    for clean in options['data_cleaning']:\n",
    "        for scaling in options['scaling']:\n",
    "            for af in options['additional_features']:\n",
    "\n",
    "                if ('In2_bool' in af) and (clean == 'remove_bad_In2_rows'):\n",
    "                    #skip this iteration\n",
    "                    continue\n",
    "\n",
    "                df_train_f = df_train_features.copy()\n",
    "                df_train_t = df_train_target.copy()\n",
    "\n",
    "                # remove bad out rows\n",
    "                df_train_f, df_train_t = remove_bad_rows_out(df_train_f, df_train_t, 10000)\n",
    "\n",
    "                if clean == 'remove_bad_In2_rows':\n",
    "                    df_train_f, df_train_t = remove_bad_In2_rows(df_train_f, df_train_t)\n",
    "\n",
    "\n",
    "                if scaling:\n",
    "                    scaler = MinMaxScaler()\n",
    "                    df_train_f.loc[:,['In2','In3']] = scaler.fit_transform(\n",
    "                        df_train_f[['In2','In3']]\n",
    "                    )\n",
    "\n",
    "                if 'interaction' in af:\n",
    "\n",
    "                    df_train_f = pd.merge(\n",
    "                        df_train_f,\n",
    "                        make_interaction_features_df(df_train_f),\n",
    "                        left_index=True,\n",
    "                        right_index=True,\n",
    "                    )\n",
    "\n",
    "                if 'In2_bool' in af:\n",
    "\n",
    "                    df_train_f = pd.merge(\n",
    "                        df_train_f,\n",
    "                        make_In2_non_linear_bool_feature_df(df_train_f),\n",
    "                        left_index=True,\n",
    "                        right_index=True,\n",
    "                    )\n",
    "\n",
    "                try:\n",
    "                    predictions = cross_val_predict(\n",
    "                        model, \n",
    "                        df_train_f, \n",
    "                        df_train_t, \n",
    "                        cv=10,\n",
    "                        verbose=0,\n",
    "                    )\n",
    "\n",
    "                    experiments += [{\n",
    "                        'model': model,\n",
    "                        'cleaning': clean,\n",
    "                        'scaling': scaling,\n",
    "                        'additional_features': af,\n",
    "                        'rmse': mean_squared_error(df_train_t, predictions, squared=False)\n",
    "                        # 'rmse_transformed_target': mean_squared_error(np.log10(df_train_target), predictions, squared=False),\n",
    "                        # 'rmse': mean_squared_error(df_train_target, (10**predictions), squared=False)\n",
    "                    }]\n",
    "                except:\n",
    "                    pass\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea48c829-0cf5-4efb-b74d-5f52a6b12fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(experiments).sort_values(['cleaning','rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea76c88-a841-45b1-aecc-0531c27677fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "823d7bdf-f9c6-4a89-ae18-e405afac54c5",
   "metadata": {},
   "source": [
    "# Comments\n",
    "\n",
    "Naturally we can't directly compare the cleaned and non-cleaned model performances due to differing number of rows, and the fact that the non-cleaned data we have a priori reason to believe may be harder due to having erroneous data.\n",
    "\n",
    "Here I'm going to focus on the models with the -1 In2 values removed, as the results makes me believe that this data was indeed erroneous as the RMSE is very low when they are not present in the data.\n",
    "\n",
    "It is interseting to note however that the models that performed best when including these values was those with the In2_bool indicator feature - which likely provided the model with the signal to treat these rows differently.\n",
    "\n",
    "It appears that the feature scaling has little to no effect, even a detrimental effect, so this preprocessing step can be ommitted.\n",
    "\n",
    "The models with interaction features consistently outperformed those without them, so these should be included.\n",
    "\n",
    "It may be that the RandomForestRegressor could do better with better hyperparameters, but with the linear model already performing well, and this being a much easier to explain model, I will progress with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def696f7-09d5-4111-83c5-ba96eedbfdb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b0b15685-2a41-4ef4-9585-80b52e66141a",
   "metadata": {},
   "source": [
    "## Deeper dive into the best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae71ce21-8eef-43d2-9f0a-98f277f69f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "\n",
    "df_train_f = df_train_features.copy()\n",
    "df_train_t = df_train_target.copy()\n",
    "\n",
    "df_train_f, df_train_t = remove_bad_rows_out(df_train_f, df_train_t, 10000)\n",
    "df_train_f, df_train_t = remove_bad_In2_rows(df_train_f, df_train_t)\n",
    "\n",
    "df_train_f = pd.merge(\n",
    "    df_train_f,\n",
    "    make_interaction_features_df(df_train_f),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb6c562-eda7-4c51-95d0-ad87e98729c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation Scores to check overfitting\n",
    "\n",
    "scores = -cross_val_score(\n",
    "    lin_reg, \n",
    "    df_train_f, \n",
    "    df_train_t, \n",
    "    cv=10,\n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "print(f\"mean rmse: {np.mean(scores)}\")\n",
    "print(f\"sd rmse: {np.std(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b24f802-1c77-4f2d-9df7-ff4cf13130a2",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "This distribution of scores suggests that the model is fairly robust to overfitting as the variance is quite low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "983a3b9a-347a-429e-9458-0dcbcc3a99c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f230e871-64cb-41e2-8211-acc32f350ae8",
   "metadata": {},
   "source": [
    "#### Now build model with a single test train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb06183e-e310-4bcd-98b8-f492d5a9df63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_x, df_val_x, df_train_y, df_val_y = train_test_split(\n",
    "    df_train_f, \n",
    "    df_train_t, \n",
    "    test_size=0.2,\n",
    "    random_state=1,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "lin_reg.fit(df_train_x, df_train_y)\n",
    "predictions = lin_reg.predict(df_val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfdb9ab-b76c-4846-8278-ffb43e70367d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction_performance(predictions, true_vals):\n",
    "    \n",
    "    plt.figure(figsize=(8,2))\n",
    "\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.scatter(predictions, true_vals, s=2, alpha=.3);\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('true value')\n",
    "    plt.title('prediction v target')\n",
    "\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.scatter(\n",
    "        np.arange(len(predictions)),\n",
    "        (predictions - true_vals), \n",
    "        s=1,\n",
    "    )\n",
    "    plt.plot(\n",
    "        [0,len(predictions)],\n",
    "        [0,0],\n",
    "        color='grey'\n",
    "    )\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('residual')\n",
    "    plt.title('residuals')\n",
    "\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.scatter(\n",
    "        np.arange(len(predictions)),\n",
    "        np.sort((predictions - true_vals)), \n",
    "        s=1,\n",
    "    )\n",
    "    plt.plot(\n",
    "        [0,len(predictions)],\n",
    "        [0,0],\n",
    "        color='grey'\n",
    "    )\n",
    "    plt.xlabel('prediction')\n",
    "    plt.ylabel('residual')\n",
    "    plt.title('residuals - ordered')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3965c45d-5657-4f61-9f3e-73dabb98d80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results\n",
    "\n",
    "plot_prediction_performance(predictions, df_val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b1faa9-4e26-4cad-9fd7-b89a6611ec72",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "As indicated previously by the RMSE, the first chart shows that this model does seem to fit the data very well and the remaining residual looks like it could be noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd402d8c-1cd0-42e5-baa6-7f4f15ad63ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e5a9486-2d17-493e-9905-42b77e06c64e",
   "metadata": {},
   "source": [
    "## Assessing performance on the hold-out test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081bbe94-7b43-47ce-8d4f-4ca74c98be5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    " \n",
    "## Preprocessing training data\n",
    "\n",
    "df_train_f = df_train_features.copy()\n",
    "df_train_t = df_train_target.copy()\n",
    "\n",
    "df_train_f, df_train_t = remove_bad_rows_out(df_train_f, df_train_t, 10000)\n",
    "df_train_f, df_train_t = remove_bad_In2_rows(df_train_f, df_train_t)\n",
    "\n",
    "df_train_f = pd.merge(\n",
    "    df_train_f,\n",
    "    make_interaction_features_df(df_train_f),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Preprocessing test data\n",
    "\n",
    "df_test_f = df_test_features.copy()\n",
    "df_test_t = df_test_target.copy()\n",
    "\n",
    "df_test_f, df_test_t = remove_bad_rows_out(df_test_f, df_test_t, 10000)\n",
    "df_test_f, df_test_t = remove_bad_In2_rows(df_test_f, df_test_t)\n",
    "\n",
    "df_test_f = pd.merge(\n",
    "    df_test_f,\n",
    "    make_interaction_features_df(df_test_f),\n",
    "    left_index=True,\n",
    "    right_index=True,\n",
    ")\n",
    "\n",
    "# Model fitting\n",
    "\n",
    "lin_reg.fit(df_train_f, df_train_t)\n",
    "predictions = lin_reg.predict(df_test_f)\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "\n",
    "plot_prediction_performance(predictions, df_test_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77c2774-1ac8-4269-943c-89530aca6553",
   "metadata": {},
   "source": [
    "### Comments\n",
    "\n",
    "Overall it looks like the model has performed well on the test set and the spread of the residuals is very similar to in training, again suggesting a model that has low variance and with low bias as the residuals seem centred around 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6653dc4e-ff61-4f3f-9f8f-b052cdf5c3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model stats from the training set\n",
    "\n",
    "df_train_f2 = sm.add_constant(df_train_f)\n",
    "est = sm.OLS(df_train_t, df_train_f2)\n",
    "est2 = est.fit()\n",
    "print(est2.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd91f61-9939-41a4-844c-faef9198dfcc",
   "metadata": {},
   "source": [
    "##### Comments\n",
    "\n",
    "The model stats seem to point towards In2 and the interaction of In1 and In4 (the two binary features) being the most important features in the model (as well as the constant term). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e7789-26af-4b2a-8fe8-853eb92b41ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
